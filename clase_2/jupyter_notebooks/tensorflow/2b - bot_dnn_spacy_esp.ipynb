{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "NV8wZ0MTKjv_"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Bot de consulta abierta y respuestas predeterminadas con DNN + Spacy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Z_9z3H-yKrcK"
   },
   "source": [
    "#### Datos\n",
    "Este ejemplo se inspiró en otro Bot en inglés creado con NLTK, lo tienen como referencia para hacer lo mismo en inglés:\\\n",
    "[LINK](https://towardsdatascience.com/a-simple-chatbot-in-python-with-deep-learning-3e8669997758)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "oCVZakCzAjGN"
   },
   "source": [
    "### 1 - Instalar dependencias\n",
    "Para poder utilizar Spacy en castellano es necesario agregar la librería \"spacy-stanza\" para lematizar palabras en español."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kzao7XO9NJAq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import string\n",
    "import random \n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_ExOb8uvjqK"
   },
   "outputs": [],
   "source": [
    "# descargaremos un pipeline de pre-procesamiento de SpaCy en español\n",
    "!python -m spacy download es_core_news_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cargar pipeline de pre-procesamiento en español\n",
    "import spacy\n",
    "nlp = spacy.load(\"es_core_news_sm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "_wF10RjVMBdV"
   },
   "source": [
    "### 2 - Herramientas de preprocesamiento de datos\n",
    "Entre las tareas de procesamiento de texto en español se implementa:\n",
    "- Quitar acentos y caracteres especiales\n",
    "- Quitar números\n",
    "- Quitar símbolos de puntuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZxoD2hEExmuX"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# El preprocesamento en castellano requiere más trabajo\n",
    "\n",
    "# Referencia de regex:\n",
    "# https://docs.python.org/3/library/re.html\n",
    "\n",
    "def preprocess_clean_text(text):    \n",
    "    # sacar tildes de las palabras:\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    # quitar caracteres especiales\n",
    "    pattern = r'[^a-zA-z0-9.,!?/:;\\\"\\'\\s]' # [^ : ningún caracter de todos estos\n",
    "    # (termina eliminando cualquier caracter distinto de los del regex)\n",
    "    text = re.sub(pattern, '', text)\n",
    "    pattern = r'[^a-zA-z.,!?/:;\\\"\\'\\s]' # igual al anterior pero sin cifras numéricas\n",
    "    # quitar números\n",
    "    text = re.sub(pattern, '', text)\n",
    "    # quitar caracteres de puntuación\n",
    "    text = ''.join([c for c in text if c not in string.punctuation])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q-MiMZjh5fu2"
   },
   "outputs": [],
   "source": [
    "text = \"personas Ideas! estás cosas y los peces y los murciélagos\"\n",
    "\n",
    "# Antes de preprocesar los datos se pasa a minúsculas todo el texto\n",
    "preprocess_clean_text(text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9V-S8JbrtNn"
   },
   "outputs": [],
   "source": [
    "# Ejemplo de como funciona\n",
    "text = \"hola personas Ideas! estás cosas y los peces y los murciélagos\"\n",
    "\n",
    "# Antes de preprocesar los datos se pasa a minúsculas todo el texto\n",
    "tokens = nlp(preprocess_clean_text(text.lower()))\n",
    "print(\"tokens:\", tokens)\n",
    "print(\"Lematización de cada token:\")\n",
    "for token in tokens:\n",
    "    print([token, token.lemma_])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ilRbn0KfMm2r"
   },
   "source": [
    "### 3 - Diccionario de entrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NgIGpjymNEH7"
   },
   "outputs": [],
   "source": [
    "# Dataset en formato JSON que representa las posibles preguntas (patterns)\n",
    "# y las posibles respuestas por categoría (tag)\n",
    "# Los \"patterns\" van a formar el corpus para entrenar el clasificador que clasifica en tags\n",
    "# \"respones\" son las respuestas predeterminadas posibles para cada tag\n",
    "dataset = {\"intents\": [\n",
    "             {\"tag\": \"bienvenida\",\n",
    "              \"patterns\": [\"Hola\", \"¿Cómo estás?\", \"¿Qué tal?\"],\n",
    "              \"responses\": [\"Hola!\", \"Hola, ¿Cómo estás?\"],\n",
    "             },\n",
    "             {\"tag\": \"nombre\",\n",
    "              \"patterns\": [\"¿Cúal es tu nombre?\", \"¿Quién sos?\"],\n",
    "              \"responses\": [\"Mi nombre es TiendaPro\", \"Yo soy TiendaPro\"]\n",
    "             },\n",
    "            {\"tag\": \"contacto\",\n",
    "              \"patterns\": [\"contacto\", \"número de contacto\", \"número de teléfono\", \"número de whatsapp\", \"whatsapp\"],\n",
    "              \"responses\": [\"Podes contactarnos al siguiente <numero>\", \"Contactos al whatsapp <numero>\"]\n",
    "             },\n",
    "            {\"tag\": \"envios\",\n",
    "              \"patterns\": [\"¿Realizan envios?\", \"¿Cómo me llega el paquete?\"],\n",
    "              \"responses\": [\"Tenemos diferentes formas de envios según la zona, te recomiendo entrar a este <link>\"]\n",
    "             },\n",
    "            {\"tag\": \"precios\",\n",
    "              \"patterns\": [\"precio\", \"Me podrás pasar los precios\", \"¿Cuánto vale?\", \"¿Cuánto sale?\"],\n",
    "              \"responses\": [\"En el siguiente link podrás encontrar los precios de todos nuestros productos en stock\"]\n",
    "             },\n",
    "            {\"tag\": \"pagos\",\n",
    "              \"patterns\": [\"medios de pago\", \"tarjeta de crédito\", \"tarjetas\", \"cuotas\"],\n",
    "              \"responses\": [\"En el siguiente link podrás encontrar los beneficios y formas de pago vigentes\"]\n",
    "             },\n",
    "            {\"tag\": \"stock\",\n",
    "              \"patterns\": [\"Esto está disponible\", \"¿Tenes stock?\", \"¿Hay stock hoy?\"],\n",
    "              \"responses\": [\"Los productos publicados están en stock\"]\n",
    "             },\n",
    "            {\"tag\": \"agradecimientos\",\n",
    "              \"patterns\": [ \"Muchas gracias\", \"Gracias\"],\n",
    "              \"responses\": [\"Por nada!, cualquier otra consulta podes escribirme\"]\n",
    "             },\n",
    "             {\"tag\": \"despedida\",\n",
    "              \"patterns\": [ \"Chau\", \"Hasta luego!\"],\n",
    "              \"responses\": [\"Hasta luego!\", \"Hablamos luego!\"]\n",
    "             }\n",
    "]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "19PEDmIDfLRu"
   },
   "source": [
    "### 4 - Preprocesamiento y armado del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3HP8abHNRk3"
   },
   "outputs": [],
   "source": [
    "# Datos que necesitaremos, las palabras o vocabulario\n",
    "words = []\n",
    "classes = []\n",
    "doc_X = []\n",
    "doc_y = []\n",
    "\n",
    "# Por cada intención (intents) debemos tomar los patrones que la caracterizan\n",
    "# a esa intención y transformarla a tokens para almacenar en doc_X\n",
    "\n",
    "# El tag de cada intención se almacena como doc_Y (la clase a predecir)\n",
    "# En `words` vamos a guardar el vocabulario\n",
    "# En `class` las posibles clases o tags\n",
    "\n",
    "for intent in dataset[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        # trasformar el patron a tokens\n",
    "        tokens = nlp(preprocess_clean_text(pattern.lower()))\n",
    "        # lematizar los tokens\n",
    "        for token in tokens:            \n",
    "            words.append(token.lemma_)\n",
    "        \n",
    "        doc_X.append(pattern)\n",
    "        doc_y.append(intent[\"tag\"])\n",
    "    \n",
    "    # Agregar el tag a las clases\n",
    "    if intent[\"tag\"] not in classes:\n",
    "        classes.append(intent[\"tag\"])\n",
    "\n",
    "# Elminar duplicados con \"set\" y ordenar el vocabulario y las clases por orden alfabético\n",
    "words = sorted(set(words))\n",
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Acy-gcugNbMH"
   },
   "outputs": [],
   "source": [
    "print(\"words:\", words)\n",
    "print(\"classes:\", classes)\n",
    "print(\"doc_X:\", doc_X)\n",
    "print(\"doc_y:\", doc_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YI0L2U7IQcvy"
   },
   "outputs": [],
   "source": [
    "# Tamaño del vocabulario\n",
    "print(\"Vocabulario:\", len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hqBeGKRk_q4r"
   },
   "outputs": [],
   "source": [
    "# Cantidad de tags\n",
    "print(\"Tags:\", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpbJ0guPN2Uq"
   },
   "outputs": [],
   "source": [
    "# Transformar doc_X en bag of words por oneHotEncoding\n",
    "# Transformar doc_Y en un vector de clases multicategórico con oneHotEncoding\n",
    "\n",
    "training = []\n",
    "out_empty = [0] * len(classes)\n",
    "\n",
    "for idx, doc in enumerate(doc_X):\n",
    "    # Transformar la pregunta (input) en tokens y lematizar\n",
    "    text = []\n",
    "    tokens = nlp(preprocess_clean_text(doc.lower()))\n",
    "    for token in tokens:\n",
    "        text.append(token.lemma_)\n",
    "\n",
    "    # Transformar los tokens en \"Bag of words\" (arrays de 1 y 0)\n",
    "    bow = []\n",
    "    for word in words:\n",
    "        bow.append(1) if word in text else bow.append(0)\n",
    "    \n",
    "    # Crear el array de salida (class output) correspondiente\n",
    "    output_row = list(out_empty)\n",
    "    output_row[classes.index(doc_y[idx])] = 1\n",
    "\n",
    "    print(\"X:\", bow, \"y:\", output_row)\n",
    "    training.append([bow, output_row])\n",
    "\n",
    "# Mezclar los datos\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "# Dividir en datos de entrada y salida\n",
    "train_X = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "a_Hr8QaDfRf3"
   },
   "source": [
    "### 5 - Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fopb3NqcAGTz"
   },
   "outputs": [],
   "source": [
    "# Shape de entrada y salida\n",
    "input_shape = (train_X.shape[1],)\n",
    "output_shape = train_y.shape[1]\n",
    "print(\"input:\", input_shape, \"output:\", output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xy7tzkwdOZx9"
   },
   "outputs": [],
   "source": [
    "# Entrenamiento del modelo DNN\n",
    "# - Modelo secuencial\n",
    "# - Con regularización\n",
    "# - softmax y optimizador Adam\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=input_shape, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(output_shape, activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"Adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r6hi4EcdOghm"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(x=train_X, y=train_y, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pb1GZDjGRP6Q"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Entrenamiento\n",
    "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTVDnrV0mDRf"
   },
   "outputs": [],
   "source": [
    "# Guardar lo necesario para poder re-utilizar este modelo en el futuro\n",
    "# En NLP además de los modelos es necesario proveer: tokenizador y vocabulario.\n",
    "# La tríada modelo+tokenizador+vocabulario es necesaria para hacer inferencia. \n",
    "# Sin una de las tres no se puede hacer inferencia.\n",
    "\n",
    "# en este caso guardamos:\n",
    "# el vocabulario utilizado (words)\n",
    "# las posibles clases\n",
    "# el modelo\n",
    "import pickle\n",
    "pickle.dump(words, open('words.pkl','wb'))\n",
    "pickle.dump(classes, open('classes.pkl','wb'))\n",
    "model.save('chatbot_model.h5')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TnD1WvhBfVYR"
   },
   "source": [
    "### 6 - Testing y validación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqBdSGt8Orkm"
   },
   "outputs": [],
   "source": [
    "# convertir texto de entrada del usuario a tokens\n",
    "def text_to_tokens(text):\n",
    "    lemma_tokens = []\n",
    "    tokens = nlp(preprocess_clean_text(text.lower()))\n",
    "    for token in tokens:\n",
    "        lemma_tokens.append(token.lemma_)\n",
    "    #print(lemma_tokens)\n",
    "    return lemma_tokens\n",
    "\n",
    "# transformar el texto de entrada tokenizado a una representación OHE\n",
    "def bag_of_words(text, vocab): \n",
    "    tokens = text_to_tokens(text)\n",
    "    bow = [0] * len(vocab)\n",
    "    for w in tokens: \n",
    "        for idx, word in enumerate(vocab):\n",
    "            if word == w: \n",
    "                bow[idx] = 1\n",
    "    #print(bow)\n",
    "    return np.array(bow)\n",
    "\n",
    "# usar modelo con la entrada en OHE y los labels posibles (tags)\n",
    "def pred_class(text, vocab, labels): \n",
    "    bow = bag_of_words(text, vocab)\n",
    "    words_recognized = sum(bow)\n",
    "\n",
    "    return_list = []\n",
    "    if words_recognized > 0: # sólo si reconoció alguna palabra del vocabulario\n",
    "        result = model.predict(np.array([bow]))[0] # es un array de softmax\n",
    "        thresh = 0.2\n",
    "        # filtrar aquellas entradas menores al umbral `thresh`\n",
    "        y_pred = [[idx, res] for idx, res in enumerate(result) if res > thresh]\n",
    "        # ordenar keys de acuerdo al valor softmax\n",
    "        y_pred.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "        # return_list es una lista de los labels de mayor a menor\n",
    "        for r in y_pred:\n",
    "            return_list.append(labels[r[0]])\n",
    "            #print(labels[r[0]], r[1])\n",
    "\n",
    "    # si no reconoció palabras del vocabulario se devuelve una lista vacía\n",
    "    return return_list\n",
    "\n",
    "# obtener una respuesta predeterminada \n",
    "def get_response(intents_list, intents_json):\n",
    "    tag = intents_list[0] # tomar el tag con el mejor valor softmax\n",
    "    list_of_intents = intents_json[\"intents\"] # intents_json es todo el dataset\n",
    "    for i in list_of_intents: \n",
    "        if i[\"tag\"] == tag: # buscar el tag correspoindiente y dar una respuesta predeterminada aleatoria \n",
    "            result = random.choice(i[\"responses\"])\n",
    "            break\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xp1vXQwdOvl7"
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    # pedir input al usuario\n",
    "    message = input(\"\")\n",
    "    print(\"Q:\", message)\n",
    "\n",
    "    intents = pred_class(message, words, classes)\n",
    "    if len(intents) > 0:\n",
    "        result = get_response(intents, dataset)\n",
    "        print(\"BOT:\", result)\n",
    "    else: # si no hubo ningún resultado que supere el umbral\n",
    "        print(\"BOT: Perdón, no comprendo la pregunta.\")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ayatkwp4fYQx"
   },
   "source": [
    "### 7 - Conclusiones\n",
    "El bot tal cual está definido es capaz de responder a bastantes tipos de preguntas con gran precisión. Algunas técnicas que podrían ensayarse para evaluar como impactan en el sistema son:\n",
    "- Filtrar los stop words\n",
    "- Utilizar TF-IDF en vez de bag of words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alumno"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tomar un ejemplo de los bots utilizados (uno de los dos) y construir el propio.\n",
    "- Sacar conclusiones de los resultados.\n",
    "\n",
    "__IMPORTANTE__: Recuerde para la entrega del ejercicio debe quedar registrado en el colab las preguntas y las respuestas del BOT para que podamos evaluar el desempeño final."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPZeXqdm2EV/WEA+xr24OAL",
   "collapsed_sections": [],
   "name": "2b - bot_dnn_spacy_esp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
