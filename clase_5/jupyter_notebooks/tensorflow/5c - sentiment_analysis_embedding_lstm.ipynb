{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBLpTr7plguX"
   },
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural\n",
    "## Sentiment analysis con Embeddings + LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9W6nuajhlqZD"
   },
   "source": [
    "### Objetivo\n",
    "El objetivo es utilizar las críticas de películas para que el sistema determine si la evaluación es positiva o negativa (sentiment analysis como clasificador binario de texto).\n",
    "En este caso vamos a usar una arquitectura basada en LSTM y Embeddings para inferir a partir de una secuencia de tamaño máximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCpOVzJdl8_p"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import io\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDbSydDfza5u"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "def curva_roc(model, X_test, y_test):\n",
    "    y_hat_prob = model.predict(X_test).ravel()\n",
    "\n",
    "    # Calcular la exactitud (accuracy)\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    print(\"Accuracy:\", scores[1])\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_hat_prob)\n",
    "    auc_keras = auc(fpr, tpr)\n",
    "    print('auc_keras', auc_keras)\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label='Keras (area = {:.3f})'.format(auc_keras))\n",
    "    plt.xlabel('False positive rate')\n",
    "    plt.ylabel('True positive rate')\n",
    "    plt.title('Curva ROC test')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UPeRkrAmbF3"
   },
   "source": [
    "### Datos\n",
    "Utilizaremos como dataset críticas de películas de IMDB puntuadas deforma positiva o negativa.\\\n",
    "Referencia del dataset: [LINK](https://www.kaggle.com/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C7jLvTU3lSyL"
   },
   "outputs": [],
   "source": [
    "# Descargar la carpeta de dataset\n",
    "!curl -L -o 'imdb_dataset.csv' 'https://drive.google.com/u/0/uc?id=1el8tS8JZhPLd2FQWPHpvG_AIyW5np3h9&export=download&confirm=t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o-SV1P3dnD1J"
   },
   "outputs": [],
   "source": [
    "# Armar el dataset\n",
    "df = pd.read_csv('imdb_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-OwSePKm-FK"
   },
   "source": [
    "### 1 - Limpieza de datos\n",
    "- En los datos se observo que en la columna \"review\" hay código HTML de salto de línea.\n",
    "- Tranformar la columna snetiment a 0 y 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-hc7-AmYnPC3"
   },
   "outputs": [],
   "source": [
    "# En los datos se observó código HTML de salto de línea <br />\n",
    "import re\n",
    "df_reviews = df.copy() \n",
    "df_reviews['review'] = df['review'].apply(lambda x: re.sub(\"<br />\", \"\", x))\n",
    "df_reviews['sentiment'] = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).values\n",
    "df_reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sZtvASVOn3ty"
   },
   "outputs": [],
   "source": [
    "# Observar como está distribuido el dataset respecto a la columna Rating\n",
    "# es decir, observar que tan balanceado se encuentra respecot a cada clase\n",
    "df_reviews['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7QJ2poZn9b-"
   },
   "outputs": [],
   "source": [
    "# Observar como está distribuido el dataset\n",
    "sns.countplot(x='sentiment', data=df_reviews)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "juVSYR89x_2v"
   },
   "source": [
    "Se puede observar que el dataset está perfectamente balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVJ_RVi4o1h3"
   },
   "outputs": [],
   "source": [
    "# Tomar la columna de las review y almacenarlo todo en un vector numpy de reviews\n",
    "text_sequences = df_reviews['review'].values\n",
    "text_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nT5Un_co65Q"
   },
   "outputs": [],
   "source": [
    "# Cuantas reviews (rows) hay para evaluar?\n",
    "len(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HP5uN9tqpHu_"
   },
   "outputs": [],
   "source": [
    "# Concatenar todas las reviews para armar el corpus\n",
    "corpus = ' '.join(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `corpus` contiene un gran string con todas las reviews concatenadas\n",
    "corpus[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FEzmePgdpf74"
   },
   "outputs": [],
   "source": [
    "# ¿Cuál es la longitud de ese corpus?\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYeJLdDmpvOe"
   },
   "outputs": [],
   "source": [
    "# Utilizar \"text_to_word_sequence\" para separar las palabras en tokens\n",
    "# recordar que text_to_word_sequence automaticamente quita los signos de puntuacion y pasa el texto a lowercase\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "tokens = text_to_word_sequence(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6L-fnWAp_lA"
   },
   "outputs": [],
   "source": [
    "# Dar un vistazo a los primeros 20 tokens/palabras\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8QgwwMUqG0d"
   },
   "outputs": [],
   "source": [
    "# ¿Cuántos tokens/palabras hay?\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFukNZdOsZ8_"
   },
   "outputs": [],
   "source": [
    "# Tokenizar las palabras con el Tokenizer de Keras\n",
    "# Definir una máxima cantidad de palabras a utilizar:\n",
    "# - num_words --> el número máximo de palabras a considerar, basado en su conteo\n",
    "# - Sólo las num_words-1 más frecuentes serán consideradas\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "num_words = 2000\n",
    "vocab_size = num_words\n",
    "tok = Tokenizer(num_words=2000) \n",
    "tok.fit_on_texts(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JnR1tlqZy94X"
   },
   "outputs": [],
   "source": [
    "# Obtener el diccionario de palabra (word) a índice y viceversa\n",
    "# y observar la cantidad total del vocabulario\n",
    "word_index = tok.word_index\n",
    "print(f'tamaño de vocabulario : {len(word_index)}')\n",
    "index_word = tok.index_word\n",
    "print(index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AvWzzSretQXf"
   },
   "outputs": [],
   "source": [
    "# Convertir las palabras/tokens a números\n",
    "sequences = tok.texts_to_sequences(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hfedPDOQD64v"
   },
   "outputs": [],
   "source": [
    "sequences[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "za73M5SRtbrP"
   },
   "outputs": [],
   "source": [
    "# Determinar el tamaño de la sequencia máxima a procesar\n",
    "\n",
    "# se puede explorar cuál es la secuencia más larga\n",
    "seqs_lenght = [len(s) for s in sequences]\n",
    "print(f'el máximo es {max(seqs_lenght)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y también explorar cómo es la distribución de tamaños de secuencias\n",
    "plt.figure()\n",
    "plt.hist(seqs_lenght,bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCkO9Wc9tls1"
   },
   "outputs": [],
   "source": [
    "# Realizar padding de las sentencias al mismo tamaño\n",
    "# tomando de referencia la máxima sentencia\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "maxlen = 200\n",
    "\n",
    "# Al realizar padding obtener la variable \"X\" (input)\n",
    "X = pad_sequences(sequences, padding='pre', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qué porcentaje de los datos se procesan completamente\n",
    "(np.array(seqs_lenght)<maxlen).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kGHHabVdt_aa"
   },
   "outputs": [],
   "source": [
    "# Observar las dimensiones de la variable input\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llVM-tzQo9_F"
   },
   "outputs": [],
   "source": [
    "# Tomar la columna rating y alcemacenarla en una variable \"y\"\n",
    "# Su shape debe ser equivalente la cantidad de rows del corpus\n",
    "y = df_reviews['sentiment'].values\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rmz9A6n4uK4V"
   },
   "outputs": [],
   "source": [
    "# Dividir los datos en train y test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EcDPlhEouQ9E"
   },
   "outputs": [],
   "source": [
    "# Determinar la dimensiones de entrada y salida\n",
    "in_shape = X_train.shape[1] # max input sentence len\n",
    "out_shape = 1 # binary classification\n",
    "print(\"in_shape\", in_shape, \", out_shape\", out_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NpbQHExL6OTu"
   },
   "source": [
    "### 2 - Entrenar el modelo con Embeddings + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUkuWBsM6cx3"
   },
   "outputs": [],
   "source": [
    "# Entrenar un modelo con LSTM entrenando sus propios embeddings\n",
    "# o utilizando embeddings pre-entrenados.\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "# input_dim = vocab_size (max n_words)\n",
    "# input_length = setencias con padding a 200\n",
    "# output_dim = 50 --> crear embeddings de tamaño 50\n",
    "model.add(Embedding(input_dim=vocab_size+1, output_dim=50, input_length=in_shape))\n",
    "model.add(LSTM(units=64, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(units=64)) # La última capa LSTM no lleva return_sequences\n",
    "\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(units=out_shape, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IiHo33J8opab"
   },
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_oTSAIjeo73-"
   },
   "outputs": [],
   "source": [
    "hist = model.fit(X_train, y_train, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-cjIatVpPqW"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "epoch_count = range(1, len(hist.history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jx1tLx23pbRi"
   },
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yr78NmybzPMP"
   },
   "outputs": [],
   "source": [
    "# Como este modelo es binario podemos calcular la curva ROC\n",
    "curva_roc(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vkkl3FTL6Uhk"
   },
   "source": [
    "### 3 - Entrenar el modelo con Embeddings Fasttext + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIgZkwiNprmG"
   },
   "outputs": [],
   "source": [
    "# Descargar los embeddings desde un gogle drive (es la forma más rápida)\n",
    "# NOTA: No hay garantía de que estos links perduren, en caso de que no estén\n",
    "# disponibles descargar de la página oficial como se explica en el siguiente bloque\n",
    "!curl -L -o 'fasttext.pkl' 'https://drive.google.com/u/0/uc?id=1Qi1r-u5lsEsNqRSxLrpNOqQ3B_ufltCa&export=download&confirm=t'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMbkn_KppuDI"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from pathlib import Path\n",
    "from io import StringIO\n",
    "import pickle\n",
    "\n",
    "class WordsEmbeddings(object):\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    def __init__(self):\n",
    "        # load the embeddings\n",
    "        words_embedding_pkl = Path(self.PKL_PATH)\n",
    "        if not words_embedding_pkl.is_file():\n",
    "            words_embedding_txt = Path(self.WORD_TO_VEC_MODEL_TXT_PATH)\n",
    "            assert words_embedding_txt.is_file(), 'Words embedding not available'\n",
    "            embeddings = self.convert_model_to_pickle()\n",
    "        else:\n",
    "            embeddings = self.load_model_from_pickle()\n",
    "        self.embeddings = embeddings\n",
    "        # build the vocabulary hashmap\n",
    "        index = np.arange(self.embeddings.shape[0])\n",
    "        # Dicctionarios para traducir de embedding a IDX de la palabra\n",
    "        self.word2idx = dict(zip(self.embeddings['word'], index))\n",
    "        self.idx2word = dict(zip(index, self.embeddings['word']))\n",
    "\n",
    "    def get_words_embeddings(self, words):\n",
    "        words_idxs = self.words2idxs(words)\n",
    "        return self.embeddings[words_idxs]['embedding']\n",
    "\n",
    "    def words2idxs(self, words):\n",
    "        return np.array([self.word2idx.get(word, -1) for word in words])\n",
    "\n",
    "    def idxs2words(self, idxs):\n",
    "        return np.array([self.idx2word.get(idx, '-1') for idx in idxs])\n",
    "\n",
    "    def load_model_from_pickle(self):\n",
    "        self.logger.debug(\n",
    "            'loading words embeddings from pickle {}'.format(\n",
    "                self.PKL_PATH\n",
    "            )\n",
    "        )\n",
    "        max_bytes = 2**28 - 1 # 256MB\n",
    "        bytes_in = bytearray(0)\n",
    "        input_size = os.path.getsize(self.PKL_PATH)\n",
    "        with open(self.PKL_PATH, 'rb') as f_in:\n",
    "            for _ in range(0, input_size, max_bytes):\n",
    "                bytes_in += f_in.read(max_bytes)\n",
    "        embeddings = pickle.loads(bytes_in)\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "    def convert_model_to_pickle(self):\n",
    "        # create a numpy strctured array:\n",
    "        # word     embedding\n",
    "        # U50      np.float32[]\n",
    "        # word_1   a, b, c\n",
    "        # word_2   d, e, f\n",
    "        # ...\n",
    "        # word_n   g, h, i\n",
    "        self.logger.debug(\n",
    "            'converting and loading words embeddings from text file {}'.format(\n",
    "                self.WORD_TO_VEC_MODEL_TXT_PATH\n",
    "            )\n",
    "        )\n",
    "        structure = [('word', np.dtype('U' + str(self.WORD_MAX_SIZE))),\n",
    "                     ('embedding', np.float32, (self.N_FEATURES,))]\n",
    "        structure = np.dtype(structure)\n",
    "        # load numpy array from disk using a generator\n",
    "        with open(self.WORD_TO_VEC_MODEL_TXT_PATH, encoding=\"utf8\") as words_embeddings_txt:\n",
    "            embeddings_gen = (\n",
    "                (line.split()[0], line.split()[1:]) for line in words_embeddings_txt\n",
    "                if len(line.split()[1:]) == self.N_FEATURES\n",
    "            )\n",
    "            embeddings = np.fromiter(embeddings_gen, structure)\n",
    "        # add a null embedding\n",
    "        null_embedding = np.array(\n",
    "            [('null_embedding', np.zeros((self.N_FEATURES,), dtype=np.float32))],\n",
    "            dtype=structure\n",
    "        )\n",
    "        embeddings = np.concatenate([embeddings, null_embedding])\n",
    "        # dump numpy array to disk using pickle\n",
    "        max_bytes = 2**28 - 1 # # 256MB\n",
    "        bytes_out = pickle.dumps(embeddings, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        with open(self.PKL_PATH, 'wb') as f_out:\n",
    "            for idx in range(0, len(bytes_out), max_bytes):\n",
    "                f_out.write(bytes_out[idx:idx+max_bytes])\n",
    "        self.logger.debug('words embeddings loaded')\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class GloveEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'glove.twitter.27B.50d.txt'\n",
    "    PKL_PATH = 'gloveembedding.pkl'\n",
    "    N_FEATURES = 50\n",
    "    WORD_MAX_SIZE = 60\n",
    "\n",
    "\n",
    "class FasttextEmbeddings(WordsEmbeddings):\n",
    "    WORD_TO_VEC_MODEL_TXT_PATH = 'cc.en.300.vec'\n",
    "    PKL_PATH = 'fasttext.pkl'\n",
    "    N_FEATURES = 300\n",
    "    WORD_MAX_SIZE = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vo3STvSgp938"
   },
   "outputs": [],
   "source": [
    "model_fasttext = FasttextEmbeddings()\n",
    "# Mirar cómo sube el uso de RAM en colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j2ECgkKCzSFt"
   },
   "outputs": [],
   "source": [
    "# Crear la Embedding matrix para la cantidad nb_words seleccionadas\n",
    "# nos vamos a quedar sólo con los vectores de embeddings para las palabras\n",
    "# que definimos como vocabulario\n",
    "\n",
    "print('preparing embedding matrix...')\n",
    "embed_dim = 300 # fasttext\n",
    "words_not_found = []\n",
    "\n",
    "# word_index proviene del tokenizer\n",
    "\n",
    "nb_words = min(num_words, len(word_index)) # vocab_size\n",
    "embedding_matrix = np.zeros((nb_words, embed_dim))\n",
    "\n",
    "# word_index está ordenado de mayor a menor frecuencia de palabras\n",
    "# las primeras que entren en el ciclo for serán las más frecuentes\n",
    "# Atención! El tokenizador de Keras no tiene soporte para stop words\n",
    "# las primeras palabras probablemente sean stopwords, para eliminarlas\n",
    "# hay que procesar el word_index o bien elegir otra librería para preprocesamiento\n",
    "# ¡ver clase 2! ;)\n",
    "for word, i in word_index.items():\n",
    "    if i >= nb_words:\n",
    "        continue\n",
    "    embedding_vector = model_fasttext.get_words_embeddings(word)[0]\n",
    "    if (embedding_vector is not None) and len(embedding_vector) > 0:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # words que no estén en el índice de la matriz de embedding tendrán\n",
    "        # como vector de embedding correspondiente todos ceros\n",
    "        words_not_found.append(word)\n",
    "        print(word)\n",
    "\n",
    "print('number of null word embeddings:', np.sum(np.sum(embedding_matrix**2, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9tChByQrQmI"
   },
   "outputs": [],
   "source": [
    "# Definimos el modelo\n",
    "\n",
    "model2 = Sequential()\n",
    "# input_dim = vocab_size (max n_words)\n",
    "# input_length = setencias con padding a 200\n",
    "# output_dim = embed_dim (depende que embeddings pre entrenados utilizamos)\n",
    "model2.add(Embedding(input_dim=vocab_size, # definido en el Tokenizador\n",
    "                     output_dim=embed_dim, # dimensión de los embeddings utilizados\n",
    "                     input_length=in_shape, # máxima sentencia de entrada\n",
    "                     weights=[embedding_matrix], # matrix de embeddings\n",
    "                    trainable=False)) # marcar como layer no entrenable\n",
    "\n",
    "model2.add(LSTM(units=64, return_sequences=True))\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(LSTM(units=64)) # La última capa LSTM no lleva return_sequences\n",
    "\n",
    "model2.add(Dense(units=128, activation='relu'))\n",
    "model2.add(Dropout(rate=0.2))\n",
    "model2.add(Dense(units=out_shape, activation='sigmoid'))\n",
    "\n",
    "model2.compile(optimizer=\"adam\",\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9s_Qx9xrzsE"
   },
   "outputs": [],
   "source": [
    "hist2 = model2.fit(X_train, y_train, epochs=5, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwfD04oXr5Sm"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "epoch_count = range(1, len(hist2.history['accuracy']) + 1)\n",
    "sns.lineplot(x=epoch_count,  y=hist2.history['accuracy'], label='train')\n",
    "sns.lineplot(x=epoch_count,  y=hist2.history['val_accuracy'], label='valid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v0OecxWJr95r"
   },
   "outputs": [],
   "source": [
    "model2.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bjcXj8n32nO"
   },
   "outputs": [],
   "source": [
    "# Como este modelo es binario podemos calcular la curva ROC\n",
    "curva_roc(model2, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGDesEf7sNrP"
   },
   "source": [
    "### 4 - Conclusión\n",
    "El modelo con embeddings pre-entrenados terminó con menos accuracy que el anterior, pero hay que tener en cuenta que este modelo no hizo overfitting y por lo tanto se lo puede seguir entrenando por más épocas o aumentar la complejidad del modelo.\n",
    "A menudo embeddings pre-entrenados de forma no supervisada como en GloVe y fasttext (basado en CBOW y Skip-Gram) no son buenas vectorizaciones de términos para problemas de sentiment analysis. Una razón de ello es que adjetivos que expresan una valoración positiva o negativa a menudo pueden compartir el mismo contexto:\n",
    "\n",
    "\"Me pareció muy **bueno** el producto.\"\n",
    "\"Me pareció muy **malo** el producto.\"\n",
    "\n",
    "Y eso hace que los vectores resultantes sean similares, lo cual es contraproducente para el problema de sentiment analysis.\n",
    "Por ello se pueden considerar las siguientes alternativas: \n",
    "- Entrenar desde cero una capa de embedding específica para el dataset de sentiment analysis a considerar, como se hizo en esta notebook.\n",
    "- Fine tuning a partir de embeddings pre-entrenados, podrían converger más rápido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
