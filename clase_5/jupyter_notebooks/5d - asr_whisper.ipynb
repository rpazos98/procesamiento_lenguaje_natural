{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ypYy7zqIwr_3ZeaVQvgCEXuIpD4iONZY","timestamp":1695321148994}],"gpuType":"T4","authorship_tag":"ABX9TyN7hseAnUuuXInlaFWS7GIA"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","# ASR con Whisper (OpenAI)\n","\n","Explorar todos los recursos disponibles en el [repositorio](https://github.com/openai/whisper) de Whisper.\n","\n"],"metadata":{"id":"yZsLFBxmADfu"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"v9anqhZiyG_9"},"outputs":[],"source":["# install whisper\n","!pip install -U openai-whisper"]},{"cell_type":"code","source":["!sudo apt install ffmpeg"],"metadata":{"id":"qhYhdpS1yUFY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import IPython"],"metadata":{"id":"9LaEp-1tziCZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import whisper\n","\n","model = whisper.load_model(\"base\")"],"metadata":{"id":"DfmbiKjiy-S-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Audio_sample = '2086-149220-0033.wav'\n","!wget https://dldata-public.s3.us-east-2.amazonaws.com/2086-149220-0033.wav\n","# Listen to it\n","IPython.display.Audio(Audio_sample)"],"metadata":{"id":"XmNR-gTt0SOW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# load audio and pad/trim it to fit 30 seconds. It becomes a numpy array.\n","audio = whisper.load_audio(Audio_sample)\n","audio = whisper.pad_or_trim(audio)"],"metadata":{"id":"4Uzut2QbzChp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# listen trimmed sample\n","IPython.display.Audio(audio, rate=15000)"],"metadata":{"id":"giT_WNSUzrWJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# make log-Mel spectrogram and move to the same device as the model\n","mel = whisper.log_mel_spectrogram(audio).to(model.device)\n","\n","# detect the spoken language\n","_, probs = model.detect_language(mel)\n","print(f\"Detected language: {max(probs, key=probs.get)}\")"],"metadata":{"id":"QudxFTBazHl9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# decode the audio\n","options = whisper.DecodingOptions()\n","result = whisper.decode(model, mel, options)\n","\n","# print the recognized text\n","print(result.text)"],"metadata":{"id":"M0M3aUaYy7Ys"},"execution_count":null,"outputs":[]}]}