{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ue5hxxkdAQJg"},"source":["<img src=\"https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Vectorización\n"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"kCED1hh-Ioyf"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"PUbfVnzIIoMj"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    return np.dot(a, b) / (np.linalg.norm(a) * (np.linalg.norm(b)))"]},{"cell_type":"markdown","metadata":{"id":"DMOa4JPSCJ29"},"source":["### Datos"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"RIO7b8GjAC17"},"outputs":[],"source":["corpus = np.array(['que dia es hoy', 'martes el dia de hoy es martes', 'martes muchas gracias'])"]},{"cell_type":"markdown","metadata":{"id":"8WqdaTmO8P1r"},"source":["Documento 1 --> que dia es hoy \\\n","Documento 2 --> martes el dia de hoy es martes \\\n","Documento 3 --> martes muchas gracias"]},{"cell_type":"markdown","metadata":{"id":"FVHxBRNzCMOS"},"source":["### 1 - Obtener el vocabulario del corpus (los términos utilizados)\n","- Cada documento transformarlo en una lista de términos\n","- Armar un vector de términos no repetidos de todos los documentos"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"3ZqTOZzDI7uv"},"outputs":[{"data":{"text/plain":["['de', 'dia', 'el', 'es', 'gracias', 'hoy', 'martes', 'muchas', 'que']"]},"execution_count":67,"metadata":{},"output_type":"execute_result"}],"source":["def extract_vocabulary(corpus):\n","    tokenized_documents = [doc.lower().split() for doc in corpus]\n","    all_tokens = [token for doc_tokens in tokenized_documents for token in doc_tokens]\n","    vocabulary = np.unique(all_tokens)\n","    vocab_list = vocabulary.tolist()\n","    return vocab_list\n","\n","vocab_list = extract_vocabulary(corpus)\n","vocab_list"]},{"cell_type":"markdown","metadata":{"id":"RUhH983FI7It"},"source":["### 2- OneHot encoding\n","Data una lista de textos, devolver una matriz con la representación oneHotEncoding de estos"]},{"cell_type":"code","execution_count":68,"metadata":{"id":"Os0AAQo6I6Z1"},"outputs":[{"data":{"text/plain":["array([[0., 1., 0., 1., 0., 1., 0., 0., 1.],\n","       [1., 1., 1., 1., 0., 1., 1., 0., 0.],\n","       [0., 0., 0., 0., 1., 0., 1., 1., 0.]])"]},"execution_count":68,"metadata":{},"output_type":"execute_result"}],"source":["\n","def one_hot_encoding(vocab, docs):\n","\n","    tokenized_documents = [doc.lower().split() for doc in corpus]\n","\n","    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","\n","    num_words = len(vocab)\n","    num_documents = len(docs)\n","    one_hot_matrix = np.zeros((num_documents, num_words))\n","\n","    for doc_idx, doc_tokens in enumerate(tokenized_documents):\n","        for token in doc_tokens:\n","            if token in word_to_index:\n","                word_idx = word_to_index[token]\n","                one_hot_matrix[doc_idx, word_idx] = 1\n","    \n","    return one_hot_matrix\n","\n","one_hot_encoding(vocab_list, corpus)"]},{"cell_type":"markdown","metadata":{"id":"IIyWGmCpJVQL"},"source":["### 3- Vectores de frecuencia\n","Data una lista de textos, devolver una matriz con la representación de frecuencia de estos"]},{"cell_type":"code","execution_count":69,"metadata":{"id":"yqij_7eHJbUi"},"outputs":[{"data":{"text/plain":["array([[0, 1, 0, 1, 0, 1, 0, 0, 1],\n","       [1, 1, 1, 1, 0, 1, 2, 0, 0],\n","       [0, 0, 0, 0, 1, 0, 1, 1, 0]])"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["def word_freq(vocab, docs):\n","\n","    num_documents = len(docs)\n","    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n","\n","    word_freq_dict = [{word: 0 for word in vocab} for _ in range(num_documents)]\n","\n","    for doc_idx, doc_tokens in enumerate(tokenized_documents):\n","        for token in doc_tokens:\n","            if token in word_to_index:\n","                word_idx = word_to_index[token]\n","                word_freq_dict[doc_idx][token] += 1\n","\n","    frequency_vectors = np.array([[word_freq_dict[doc_idx][word] for word in vocab] for doc_idx in range(num_documents)])\n","\n","    return frequency_vectors\n","\n","word_freq(vocab_list, corpus)"]},{"cell_type":"markdown","metadata":{"id":"z_Ot8HvWJcBu"},"source":["### 4- TF-IDF\n","Data una lista de textos, devolver una matriz con la representacion TFIDF"]},{"cell_type":"code","execution_count":70,"metadata":{"id":"waG_oWtpJjRw"},"outputs":[{"data":{"text/plain":["array([[0.        , 0.40546511, 0.        , 0.40546511, 0.        ,\n","        0.40546511, 0.        , 0.        , 1.09861229],\n","       [1.09861229, 0.40546511, 1.09861229, 0.40546511, 0.        ,\n","        0.40546511, 0.81093022, 0.        , 0.        ],\n","       [0.        , 0.        , 0.        , 0.        , 1.09861229,\n","        0.        , 0.40546511, 1.09861229, 0.        ]])"]},"execution_count":70,"metadata":{},"output_type":"execute_result"}],"source":["def calculate_tfidf(vocab, docs):\n","    \n","    num_documents = len(docs)\n","    num_words = len(vocab)\n","    \n","    tf_matrix = word_freq(vocab, docs)\n","    idf_vector = np.array([np.log(num_documents / np.count_nonzero(tf_matrix[:, word_idx])) for word_idx in range(num_words)])\n","    tfidf_matrix = tf_matrix * idf_vector\n","    \n","    return tfidf_matrix\n","\n","tfidf_matrix = calculate_tfidf(vocab_list, corpus)\n","tfidf_matrix"]},{"cell_type":"markdown","metadata":{"id":"xMcsfndWJjm_"},"source":["### 5 - Comparación de documentos\n","Realizar una funcion que reciba el corpus y el índice de un documento y devuelva los documentos ordenados por la similitud coseno"]},{"cell_type":"code","execution_count":75,"metadata":{"id":"CZdiop6IJpZN"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Document: que dia es hoy\n","Similar Document 1: martes el dia de hoy es martes\n","Similar Document 2: martes muchas gracias\n"]}],"source":["def sort_documents_by_cosine_similarity(corpus, tfidf_matrix, doc_index):\n","    doc_tfidf = tfidf_matrix[doc_index]\n","    cosine_similarities = [cosine_similarity(doc_tfidf, tfidf_matrix[i]) for i in range(len(corpus))]\n","    sorted_indices = np.argsort(cosine_similarities)[::-1]\n","    sorted_indices = sorted_indices[sorted_indices != doc_index]\n","    sorted_documents = [corpus[idx] for idx in sorted_indices]\n","    return sorted_documents\n","\n","\n","document_index = 0\n","similar_documents = sort_documents_by_cosine_similarity(corpus, tfidf_matrix, document_index)\n","\n","print(f\"Original Document: {corpus[document_index]}\")\n","\n","for idx, doc in enumerate(similar_documents, start=1):\n","    print(f\"Similar Document {idx}: {doc}\")"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyO5fRYTpympAwJSVbric6dW","collapsed_sections":[],"name":"1a - word2vec.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
