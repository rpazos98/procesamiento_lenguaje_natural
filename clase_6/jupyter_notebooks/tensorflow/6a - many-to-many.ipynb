{"cells":[{"cell_type":"markdown","metadata":{"id":"pfa39F4lsLf3"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## LSTM many-to-many"]},{"cell_type":"markdown","metadata":{"id":"ZqO0PRcFsPTe"},"source":["### Datos\n","El objecto es utilizar una serie de sucuencias númericas (datos sintéticos) para poner a prueba el uso de las redes LSTM. Este ejemplo se inspiró en otro artículo, lo tienen como referencia en el siguiente link:\\\n","[LINK](https://stackabuse.com/solving-sequence-problems-with-lstm-in-keras-part-2/)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cq3YXak9sGHd"},"outputs":[],"source":["import re\n","\n","import numpy as np\n","import pandas as pd\n","\n","from keras.preprocessing.text import one_hot\n","from keras.models import Sequential\n","from keras.layers import Activation, Dropout, Dense\n","from keras.layers import Flatten, LSTM, SimpleRNN\n","from keras.models import Model\n","from tensorflow.keras.layers import Embedding\n","from sklearn.model_selection import train_test_split\n","from keras.preprocessing.text import Tokenizer\n","from keras.layers import Input\n","from keras.layers import Bidirectional"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9aNLZBDtA5J"},"outputs":[],"source":["# Generar datos sintéticos\n","X = list()\n","y = list()\n","\n","# En ambos casos \"X\" e \"y\" son vectores de números de 5 en 5\n","X = [x for x in range(5, 301, 5)]\n","y = [x+15 for x in X]\n","\n","print(f\"datos X (len={len(X)}):\", X)\n","print(f\"datos y (len={len(y)}):\", y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ACkrI0GtEAM"},"outputs":[],"source":["# Se desea agrupar los datos de a 3 elementos\n","X = np.array(X).reshape(len(X)//3, 3, 1)\n","y = np.array(y).reshape(len(y)//3, 3, 1)\n","print(\"datos X[0:2]:\", X[0:2])\n","print(\"datos y[0:2]:\", y[0:2])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ajykLcJ8tFfN"},"outputs":[],"source":["# Verificamos que la secuencia de entrada es igual a la secuencia de salida\n","# en cuanto a dimensiones\n","# Tendremos:\n","#  --> veinte grupos de datos (rows) (20)\n","#  --> cada grupo compuesto por tres elementos (3)\n","#  --> cada elemento representado en una sola dimension (1)\n","print(\"X shape:\", X.shape)\n","print(\"y shape:\", y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lqznaoFYyDZC"},"outputs":[],"source":["# Cardinalidad (cantidad de elementos distintos en el dataset)\n","data = np.append(X, y)\n","len(np.unique(data))"]},{"cell_type":"markdown","metadata":{"id":"3vKbhjtIwPgM"},"source":["### 2 - Entrenar el modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vIhN4zRgwMtE"},"outputs":[],"source":["input_shape = X[0].shape\n","input_shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xNxu30D3wVAA"},"outputs":[],"source":["output_shape = y[0].shape\n","output_shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYr0PLxgmM45"},"outputs":[],"source":["model = Sequential()\n","\n","# input LSTM layer\n","# Aquí se transformar las entradas en features\n","# Retornarmos la secuencia para que la salida tenga la siguiente dimension:\n","#   --> (tamaño batch, tamaño serie, tamaño elemento)\n","# A diferencia de otra veces, estamos agregando el tamaño de la secuencia/serie\n","model.add(LSTM(128, activation='relu', input_shape=(input_shape), return_sequences=True))\n","\n","# Al final tengo una salida (una secuencia) de 3 elementos juntos, \n","# cada elemento de dimension 1:\n","# --> (3x1)\n","model.add(Dense(output_shape[-1]))\n","\n","# Notar que en este caso la salida es con activación lineal\n","# estamos considerando el problema de inferencia como una regresión\n","# Esto será distinto cuando encaremos problemas de texto,\n","# en donde la salida serán términos discretos.\n","\n","model.compile(loss='mse',\n","              optimizer=\"Adam\")\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xFPJeRu9zMM9"},"outputs":[],"source":["from keras.utils import plot_model\n","plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"]},{"cell_type":"markdown","metadata":{"id":"td9b0gySzW8D"},"source":["Esta arquitectura cumple lo solicitado, pero es bastante limitada y rebuscada. En el futuro y otros ejemplos veremos la arquitectura tipo encoder-decoder que es más flexible y \"simétrica\" que la utilizada en este caso."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VnlIx1Vezjwc"},"outputs":[],"source":["hist = model.fit(X, y, epochs=500, validation_split=0.2, batch_size=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OVz1uug_zu2J"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Entrenamiento\n","epoch_count = range(1, len(hist.history['loss']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history['loss'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist.history['val_loss'], label='valid')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C4_UQshIzw7o"},"outputs":[],"source":["# Ensayo\n","x_test = [20, 25, 30]\n","y_test = [x+15 for x in x_test]\n","\n","test_input = np.array([x_test])\n","test_input = test_input.reshape((1, 3, 1))\n","y_hat = model.predict(test_input, verbose=0)[0]\n","\n","print(\"y_test:\", y_test)\n","print(\"y_hat:\", y_hat[0], y_hat[1], y_hat[2])\n","\n","model.evaluate(test_input, np.array([y_test]))"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6"},"vscode":{"interpreter":{"hash":"916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"}}},"nbformat":4,"nbformat_minor":0}
