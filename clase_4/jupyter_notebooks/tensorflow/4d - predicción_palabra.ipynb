{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"g3yeJGnCYxuF"},"source":["<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n","\n","\n","# Procesamiento de lenguaje natural\n","## Predicción de próxima palabra"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Iv5PEwGzZA9-"},"source":["### Objetivo\n","El objetivo es utilizar documentos / corpus para crear embeddings de palabras basado en ese contexto utilizando la layer Embedding de Keras. Se utilizará esos embeddings junto con layers LSTM para predeccir la próxima posible palabra."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-QdFbHZYj7C"},"outputs":[],"source":["import random\n","import io\n","import pickle\n","\n","import numpy as np\n","import pandas as pd\n","\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from keras.utils import to_categorical\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Embedding, Dropout"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xTvXlEKQZdqx"},"source":["### Datos\n","Utilizaremos como dataset canciones de bandas de habla inglés."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IkdPfrQJZdB5"},"outputs":[],"source":["# Descargar la carpeta de dataset\n","import os\n","import platform\n","if os.access('./songs_dataset', os.F_OK) is False:\n","    if os.access('songs_dataset.zip', os.F_OK) is False:\n","        if platform.system() == 'Windows':\n","            !curl https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/datasets/songs_dataset.zip -o songs_dataset.zip\n","        else:\n","            !wget songs_dataset.zip https://github.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/raw/main/datasets/songs_dataset.zip\n","    !unzip -q songs_dataset.zip   \n","else:\n","    print(\"El dataset ya se encuentra descargado\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6j-3nQ4lZjfb"},"outputs":[],"source":["# Posibles bandas\n","os.listdir(\"./songs_dataset/\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gb39v3PaZmRH"},"outputs":[],"source":["# Armar el dataset utilizando salto de línea para separar las oraciones/docs\n","df = pd.read_csv('songs_dataset/beatles.txt', sep='/n', header=None)\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"riT898QlZnmF"},"outputs":[],"source":["print(\"Cantidad de documentos:\", df.shape[0])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RDoouHp7Zp6D"},"source":["### 1 - Ejemplo de Preprocesamiento\n","- Hay que transformar las oraciones en tokens.\n","- Dichas oraciones hay que ajustarlas al tamaño fijo de nuestra sentencia de entrada al modelo.\n","- Hay que separar las palabras objetivos (target) que el modelo debe predecir en cada sentencia armada."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m5FeTaGvbDbw"},"outputs":[],"source":["from keras.preprocessing.text import Tokenizer # equivalente a ltokenizer de nltk\n","from keras.preprocessing.text import text_to_word_sequence # equivalente a word_tokenize de nltk\n","from keras.utils import pad_sequences # se utilizará para padding\n","\n","# largo de la secuencia, incluye seq input + word output\n","train_len = 4"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zf3O7eK6ZpP8"},"outputs":[],"source":["# Ejemplo de como transformar una oración a tokens usando keras\n","text = df.loc[0,0]\n","text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AOv67Sj7aeFH"},"outputs":[],"source":["tokens = text_to_word_sequence(text) # entran oraciones -> salen vectores de N posiciones (tokens)\n","tokens"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZrlyqkoiaymK"},"source":["1.1 - Transformar las oraciones en secuencias (tokens) de palabras"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XH_L14Wjaowe"},"outputs":[],"source":["# Recorrer todas las filas y transformar las oraciones\n","# en secuencias de palabras\n","sentence_tokens = []\n","for _, row in df[:None].iterrows():\n","    sentence_tokens.append(text_to_word_sequence(row[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KASzU4CdaxbZ"},"outputs":[],"source":["# Demos un vistazo\n","sentence_tokens[:2]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A659lswTbIIB"},"outputs":[],"source":["# Código para hacer el desfasaje de las palabras\n","# según el train_len\n","text_sequences = []\n","\n","for i in range(train_len, len(tokens)):\n","  seq = tokens[i-train_len:i]\n","  text_sequences.append(seq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"01JEoPPnbgRF"},"outputs":[],"source":["# Demos un vistazo a nuestros vectores para entrenar el modelo\n","text_sequences "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4B0gHnKVa4W_"},"source":["1.2 - Crear los vectores de palabras (word2vec)\n","\n","Ahora necesitamos pasarlos a números para que lo entienda la red y separar input de output.\n","- El Input seran integers (word2vec)\n","- Mientras que el output será one hot encodeado (labels) del tamaño del vocabulario"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fkPNvXeQcS0U"},"outputs":[],"source":["tok = Tokenizer() \n","\n","# El tokenizer \"aprende\" las palabras que se usaran\n","# Se construye (fit) una vez por proyecto, se aplica N veces (tal cual un encoder)\n","# El token 0 es reservado y no es asignado, esta sutileza nos dará un pequeño\n","# problema al momento de hacer la codificación OHE, pero es sencillo de resolver\n","tok.fit_on_texts(text_sequences) \n","\n","# Convertimos las palabras a números\n","# entran palabras -> salen números\n","sequences = tok.texts_to_sequences(text_sequences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4SIc44IocyQb"},"outputs":[],"source":["# Ahora sequences tiene los números \"ID\", largo 4\n","sequences"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ro81yCQc1oX"},"outputs":[],"source":["# Cantidad de casos (doc) de entrada\n","print(tok.document_count)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nzAWNfroc4u1"},"outputs":[],"source":["# Cantidad de veces que aparece cada palabra\n","print(len(tok.word_counts))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"spTBxmFQc6h8"},"outputs":[],"source":["# El índice para cada palabra\n","# El sistema las ordena de las más populares a las menos populares\n","print(tok.word_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nUDkjy80c77h"},"outputs":[],"source":["# Cantidad de veces quea aparece cada palabra en cada \"documento\"\n","# (1 documento = 1 caso de entrada)\n","print(tok.word_docs)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ohS5Tao1d2KB"},"source":["### 2 - Preprocesamiento completo\n","Debemos realizar los mismos pasos que en el ejemplo anterior, pero antes de eso debemos transformar ese dataset de filas de oraciones en un texto completo continuo para poder extraer el vocabulario."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"63Z2-Se2t27r"},"outputs":[],"source":["# Vistazo a las primeras filas\n","df.loc[:15,0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kILsSoxTuHEr"},"outputs":[],"source":["# Concatenamos todos los rows en un solo valor\n","corpus = df.apply(lambda row: ' '.join(row.values.astype(str)), axis=0)[0]\n","corpus"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KlsYd7_uOez"},"outputs":[],"source":["# Transformar el corpus a tokens\n","tokens=text_to_word_sequence(corpus)\n","# Vistazo general de los primeros tokens\n","tokens[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GlqpZSJOJ1xQ"},"outputs":[],"source":["print(\"Cantidad de tokens en el corpus:\", len(tokens))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RhQevOynuYk2"},"outputs":[],"source":["# Código para hacer el desfasaje de las palabras\n","# según el train_len\n","text_sequences = []\n","for i in range(train_len, len(tokens)):\n","  seq = tokens[i-train_len:i]\n","  text_sequences.append(seq)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FU3FuqHSuhzq"},"outputs":[],"source":["# Demos un vistazo a nuestros vectores para entrenar el modelo\n","text_sequences[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"064N2jtLvHRg"},"outputs":[],"source":["# Proceso de tokenización\n","tok = Tokenizer() \n","tok.fit_on_texts(text_sequences) \n","\n","# Convertimos las palabras a números\n","# entran palabras -> salen números\n","sequences = tok.texts_to_sequences(text_sequences)\n","\n","# Damos un vistazo\n","sequences[:20]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vwsvmvDKKXSP"},"outputs":[],"source":["print(\"Cantidad de rows del dataset:\", len(sequences))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QMVP4bj0vL2e"},"source":["### 3 - Input y target"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mx2xwdz3KloJ"},"outputs":[],"source":["# Con numpy es muy fácil realizar el slicing de vectores\n","ex = np.array([[1,2,3,4],[5,6,7,8]])\n","ex"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEod7qghvTVt"},"outputs":[],"source":["# Con numpy es muy fácil realizar el slicing de vectores\n","print(\"Dimension:\", ex.shape)\n","print(\"Todos los elementos:\", ex)\n","print(\"Todos los elementos menos el último:\", ex[:, :-1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i95xWqtCvp8T"},"outputs":[],"source":["input = ex[:,:-1] # todos los rows, menos la ultima col\n","target = ex[:, -1] # última col de cada row\n","\n","print(\"Input:\", input)\n","print(\"Target:\", target)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1vJTG65v4Qn"},"outputs":[],"source":["arr_sequences = np.array(sequences)\n","x_data = arr_sequences[:,:-1]\n","y_data_int = arr_sequences[:,-1] # aún falta el oneHotEncoder\n","\n","print(x_data.shape)\n","print(y_data_int.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ln6kVWVlwBBs"},"outputs":[],"source":["# Palabras del vocabulario\n","tok.index_word"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gJgVhq1zwEpf"},"outputs":[],"source":["# Cantidad de palabras en el vocabulario\n","vocab_size = len(tok.word_counts)\n","vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wKV85M2uwKJ9"},"outputs":[],"source":["# Transformar los datos a oneHotEncoding\n","y_data = to_categorical(y_data_int, num_classes=vocab_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWjNrNx9wM1b"},"outputs":[],"source":["# En el caso anterior explota porque y_data_int comienza en \"1\" en vez de \"0\"\n","# valor minimo:\n","min(y_data_int)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gIg2e2WCwXbG"},"outputs":[],"source":["y_data_int_offset = y_data_int - 1\n","y_data = to_categorical(y_data_int_offset, num_classes=vocab_size) \n","y_data.shape"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GmJWNyxQwfCE"},"source":["### 4 - Entrenar el modelo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0cOmNZT_weK2"},"outputs":[],"source":["# largo de la secuencia de entrada\n","input_seq_len = x_data.shape[1] \n","input_seq_len"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtwITjgnwlgp"},"outputs":[],"source":["# Largo del vector de salida --> vocab_size\n","output_size = vocab_size\n","output_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jzTZRXrrwrvi"},"outputs":[],"source":["model = Sequential()\n","\n","# Embedding:\n","# input_seq_len = 3 --> ingreso 3 palabras\n","# input_dim = vocab_size --> 1628 palabras distintas\n","# output_dim = 5 --> crear embeddings de tamaño 3 (tamaño variable y ajustable)\n","model.add(Embedding(input_dim=vocab_size+1, output_dim=5, input_length=input_seq_len))\n","\n","model.add(LSTM(64, return_sequences=True))\n","model.add(Dropout(0.2))\n","model.add(LSTM(64)) # La última capa LSTM no lleva return_sequences\n","model.add(Dense(32, activation='relu'))\n","\n","# Predicción de clasificación con softmax\n","# La salida vuelve al espacio de 1628 palabras posibles\n","model.add(Dense(vocab_size, activation='softmax'))\n","\n","# Clasificación multiple categórica --> loss = categorical_crossentropy\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oQq1PHDkxDvN"},"outputs":[],"source":["hist = model.fit(x_data, y_data, epochs=50, validation_split=0.2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q_orBXOrCsNn"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Entrenamiento\n","epoch_count = range(1, len(hist.history['accuracy']) + 1)\n","sns.lineplot(x=epoch_count,  y=hist.history['accuracy'], label='train')\n","sns.lineplot(x=epoch_count,  y=hist.history['val_accuracy'], label='valid')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KN6Fg_BsxJe6"},"source":["### 5 - Predicción de próxima palabra"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iy_AXWQWzeeE"},"outputs":[],"source":["# Keras pad_sequences\n","# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences\n","# Si la secuencia de entrada supera al input_seq_len (3) se trunca\n","# Si la secuencia es más corta se agregna ceros al comienzo"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IBvKHFPmzpy2"},"outputs":[],"source":["# Se utilizará gradio para ensayar el modelo\n","# Herramienta poderosa para crear interfaces rápidas para ensayar modelos\n","# https://gradio.app/\n","import sys\n","!{sys.executable} -m pip install gradio --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HNyBykvhzs7-"},"outputs":[],"source":["import gradio as gr\n","\n","def model_response(human_text):\n","\n","    # Encodeamos\n","    encoded = tok.texts_to_sequences([human_text])[0]\n","    # Si tienen distinto largo\n","    encoded = pad_sequences([encoded], maxlen=3, padding='pre')\n","    \n","    # Predicción softmax\n","    y_hat = model.predict(encoded).argmax(axis=-1)\n","\n","    # Debemos buscar en el vocabulario la palabra\n","    # que corresopnde al indice (y_hat) predicho por le modelo\n","    out_word = ''\n","    for word, index in tok.word_index.items():\n","        if index == y_hat:\n","            out_word = word\n","            break\n","\n","    # Agrego la palabra a la frase predicha\n","    return human_text + ' ' + out_word\n","\n","iface = gr.Interface(\n","    fn=model_response,\n","    inputs=[\"textbox\"],\n","    outputs=\"text\",\n","    layout=\"vertical\")\n","\n","iface.launch(debug=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"mCeMWWupxN1-"},"source":["### 6 - Generación de secuencias nuevas"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bwbS_pfhxvB3"},"outputs":[],"source":["def generate_seq(model, tokenizer, seed_text, max_length, n_words):\n","    \"\"\"\n","        Exec model sequence prediction\n","\n","        Args:\n","            model (keras): modelo entrenado\n","            tokenizer (keras tokenizer): tonenizer utilizado en el preprocesamiento\n","            seed_text (string): texto de entrada (input_seq)\n","            max_length (int): máxima longitud de la sequencia de entrada\n","            n_words (int): números de palabras a agregar a la sequencia de entrada\n","        returns:\n","            output_text (string): sentencia con las \"n_words\" agregadas\n","    \"\"\"\n","    output_text = seed_text\n","\t# generate a fixed number of words\n","    for _ in range(n_words):\n","\t\t# Encodeamos\n","        encoded = tokenizer.texts_to_sequences([output_text])[0]\n","\t\t# Si tienen distinto largo\n","        encoded = pad_sequences([encoded], maxlen=max_length, padding='pre')\n","\t\t\n","\t\t# Predicción softmax\n","        y_hat = model.predict(encoded).argmax(axis=-1)\n","\t\t# Vamos concatenando las predicciones\n","        out_word = ''\n","\n","        # Debemos buscar en el vocabulario la palabra\n","        # que corresopnde al indice (y_hat) predicho por le modelo\n","        for word, index in tokenizer.word_index.items():\n","            if index == y_hat:\n","                out_word = word\n","                break\n","\n","\t\t# Agrego las palabras a la frase predicha\n","        output_text += ' ' + out_word\n","    return output_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JoFqRC5pxzqS"},"outputs":[],"source":["input_text='hey jude don\\'t'\n","\n","generate_seq(model, tok, input_text, max_length=3, n_words=2)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"T2SHmXbgxQH9"},"source":["### 7 - Conclusiones\n","El modelo entrenado tuvo un muy mail desempeño en el entrenamiento además de overfitting. Cuestiones que podrían mejorarse:\n","- Agregar más capas o neuronaes\n","- Incrementar la cantidad de épocas\n","- Agregar BRNN\n","\n","Es importante destacar que en este ejemplo estamos entrenando nuestro propios Embeddings, y para ello se requiere mucha data. En los ejemplos que realizaremos de aquí en más utilizaremos más datos, embeddings pre-enternados o modelos pre-entrenados."]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOkfjJIpCd091Vpf71hUImQ","collapsed_sections":[],"name":"4d - predicción_palabra.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
